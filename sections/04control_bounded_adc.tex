% !TEX root = ../report.tex

\chapter{Control-Bounded A/D Conversion}
\label{sec:cbadc}
\section{History and Background}
Control-bounded A/D conversion is a conceptually new approach to the problem of creating a digital representation of an analog signal. The conversion technique has developed over the last years, and the progress is mainly pushed forward by prof. Hans-Andrea Loeliger et al., from the Signal and Information Processing Laboratory (ISI), ETH ZÃ¼rich. The concept was first introduced at the IEEE Information Theory \& Applications Workshop (ITA), february 2011 \cite{cbc_2011_loeliger}. In this paper, the main building blocks of a control-bounded ADC was presented, but no explicit example of such an ADC was given, and no behavioural analysis presented. The approach was further developed in \cite{cbc_2015_loeliger}, which was published for the same conference in 2015. In this paper, the conversion algorithm is improved and a limited transfer function analysis is presented. The latest publication on control-bounded conversion is from 2020 \cite{cbc_2020_loeliger}. This is a longer paper with the goal of providing the sufficient information for analog designers to experiment with control-bounded ADCs. The paper provides a more details on the operation of the building blocks, together with a full transfer function analysis. Measurements on a proof-of-concept hardware prototype is also presented.

In addition to the mentioned papers, Hampus Malmberg, co-author of the latest paper \cite{cbc_2020_loeliger}, has recently defended his Ph.D. on control-bounded converters. The thesis\cite{malmberg_thesis} is not yet published, but the author has been given access to a draft which serves as the main source of information on this topic. Malmberg also held a presentation at the 2020 IEEE International Symposium on Circuits and Systems (ISCAS)\cite{malmberg_talk}, where he presented the basic concept of control-bounded ADC, together with the Hadamard ADC which is a basis for the architecture proposed in a later chapter.

In this section, the operating principle of a control-bounded converter is described in detail, and we follow the notation established in \cite{cbc_2020_loeliger}. The theoretical presentation given in this section will be very close to that of \cite{malmberg_thesis}, but less general and limited to what is necessary for understanding the the rest of the thesis.


























\section{Overview}
\label{sec:cbc_overview}
The control-bounded ADC approaches the A/D conversion problem differently compared to conventional A/D converters. The conceptual difference lies in the view on sampling. In a control-bounded converter, the analog input signal is never sampled in the traditional way. The circuit that constitutes a control-bounded ADC still contains quantizers, but the quantized signals are never treated as a sampled and quantized version of the input. Instead, they are intermediate digital signals that only indirectly relates to the input, and they are used by a digital estimation filter to perform the digital estimate of the input signal.

To clarify this, consider the general block diagram shown in figure \ref{fig:cbadc1}.
\begin{figure}[htbp]
    \centering
    \input{figures/03theory/cbadc1.tex}
    \caption{A block diagram of the control-bounded ADC. Figure from \cite{malmberg_thesis}.}
    \label{fig:cbadc1}
\end{figure}

As the figure indicates, the control-bounded ADC consists of three main building blocks; an analog system (AS), a digital control (DC) and a digital estimator (DE). The signals $\utv, \utvhat, \stvtilde, \skv$ and $\stv$ are in general vector-valued functions. The analog system amplifies the input signal $\utv$, preferably with very high gain within the frequency band of interest. The digital control stabilizes the analog system by forcing the internal states of the system to stay within its bounds. The internal states are observed through the \textit{control observation} $\stvtilde$ and controlled through the \textit{control contribution} $\stv$. The digital estimator takes the \textit{control signal} $\skv$ as an input and forms the digital estimate $\utvhat$ of $\utv$.

Note that the output of the digital estimator is denoted as a continuous-time estimate $\utvhat$ instead of a discrete-time estimate $\ukvhat$. The digital estimator models the continuous-time dynamics of the analog system, and is thereby capable of estimating $\utv$ at arbitrary time instances. The actual estimates will obviously be computed at discrete time steps, but because the digital estimator itself imposes no criteria on this time interval, the output is denoted as a continuous time estimate.

\begin{figure}[htbp]
    \centering
    \input{figures/03theory/cbadc2.tex}
    \caption{A control-bounded ADC with the DC box opened}
    \label{fig:cbadc2}
\end{figure}

Before going into detail on each of these building blocks, consider figure \ref{fig:cbadc2}, which shows the same block diagram, but with the DC box opened. From this figure it is evident that the structure of the control-bounded ADC is very similar to that of the continuous-time $\sd$ modulator in figure \ref{fig:ctsdmod}. The control-bounded converter may in fact be viewed as a generalization of the continuous-time $\sd$ ADC. As mentioned, the main difference between these architectures arises from the different interpretation of the control signal $\skv$. In the $\sd$ ADC, this signal is viewed as a filtered, sampled and quantized version of the input signal and the digital output is obtained by averaging this signal through a decimation filter. In the control-bounded perspective, the direct relation between $\skv$ and $\utv$ is ignored completely. Instead, we focus solely on the fact that $\stv$ is the contribution needed to stabilize the internal states of the analog system. This view leads to a different estimation filter for the reconstruction of $\utvhat$.

It should be noted that the contribution of the control-bounded ADC is not to provide an alternative decimation filter to already existing $\sd$ ADCs. Stepping away from the sampling oriented view of $\skv$, relaxes the requirements on the parts of the circuit that produces this signal. More importantly, the indirect relation between $\skv$ and $\utv$ enables a more general interaction between the analog and the digital parts of the system. There are for instance no need to require a one-to-one relation between the elements of these two signals, and they need not be of the same dimension. Each element of $\skv$ might contain information from all elements of $\utv$. These features are particularly interesting for multi-cannel systems which is of special interest in this thesis.


















\section{Analog System}
\label{subsec:analog_system}
The analog system, here assumed to be a continuous time filter, sets the frequency response of the overall ADC, and is designed to amplify the frequency band of interest. As stability of the analog system is controlled digitally, the analog system itself need not be stable.

\subsection{State Space Model}
The dynamics of the analog system is described using a state space model notation, illustrated in figure \ref{fig:as_state_space_model}.
\begin{figure}[htbp]
    \input{figures/03theory/as_state_space_model.tex}
    \centering
    \caption{State space model of the AS. Figure from \cite{malmberg_thesis}.}
    \label{fig:as_state_space_model}
\end{figure}
The multi-channel input signal $\utv$, the state-vector $\xtv$ and the control contribution $\stv$ is related by the differential equation
\begin{equation}
    \label{eq:state_space_equations}
    \dot{\bm{x}}(t) = \Amat\xtv + \Bmat\utv + \Gmat\stv.
\end{equation}
where
\begin{equation}
    \label{eq:def_input_sig}
    \utv \triangleq \left( u_1(t),..., u_L(t) \right)^\mathsf{T} \in \R^L,
\end{equation}
\begin{equation}
    \label{eq:def_state_vector}
    \xtv \triangleq \left( x_1(t),..., x_N(t) \right)^\mathsf{T} \in \R^N
\end{equation}
and
\begin{equation}
    \label{eq:def_control_contribution}
    \stv \triangleq \left( s_1(t),..., s_M(t) \right)^\mathsf{T} \in \R^M.
\end{equation}
This system is said to have $L$ input channels, $M$ controls and $N$ states. We will refer to $\Amat \in \R^{N \times N}$, $\Bmat \in \R^{N \times L}$ and $\Gmat \in \R^{N \times M}$ as the \textit{system matrix}, the \textit{input matrix} and \textit{the control input matrix} respectively. We will also refer to $N$ as the \textit{system order}.

The only physical output of the analog system is the control observation
\begin{equation}
    \label{eq:def_control_observation}
    \stvtilde \triangleq \GmattildeT \xtv \in \R^{M},
\end{equation}
which is used by the digital control to produce the control signal $\skv$. The control observation is a linear mapping of the internal state-vector, through the \textit{control observation matrix} $\GmattildeT \in \R^{M \times N}$.
The second output of the analog system is the purely conceptual signal
\begin{equation}
    \label{eq:def_as_output}
    \ytv \triangleq \CmatT \xtv \in \R^{\tilde{N}},
\end{equation}
 which is used by the digital estimator to produce the estimate $\utvhat$. This signal has no physical meaning, and the \textit{signal observation matrix} $\CmatT \in \R^{\tilde{N} \times N}$ is basically telling the digital estimation algorithm which of the internal analog states that could be treated as bounded. Thus $\ytv$ and $\CmatT$ does only exist conceptually inside the digital estimator.

\subsection{Transfer Function and Impulse Response Matrix}
\label{sec:as_tf}
The transfer function of the analog system gives the frequency domain relation between the input $\bm{U}(\omega)$ and the output $\bm{Y}(\omega)$. Hence for the general case of $L$ input channels and $\tilde{N}$ outputs, the analog transfer function (ATF) is a $\tilde{N}$-by-$L$ matrix, defined by $\bm{Y}(\omega) = \Gw \bm{U}(\omega)$. Each element $G_{i,j}(\omega)$ of $\Gw$ is the transfer function from $U_j(\omega)$ to $Y_i(\omega)$. From (\ref{eq:state_space_equations}) the ATF is obtained as
\begin{equation}
    \label{eq:ATF_def}
    \Gw = \CmatT\left(\jw \eyen{N} - \Amat \right)^{-1}\Bmat,
\end{equation}
and the derivation is given in appendix \ref{a:ATF}. The analog impulse response matrix is then obtained from the inverse Laplace transform as
\begin{equation}
    \bm{g}(t) = \CmatT \exp(\Amat t) \Bmat,
\end{equation}
where $\exp$ denotes the matrix exponential.






























\section{Digital Control}
\label{sec:04digcontrol}
The digital control is a discrete time system which serves the purpose of stabilizing the analog system. It includes a sample-and-hold circuit, a one-bit quantizer and a D/A converter, as shown in figure \ref{fig:cbadc2}. The control observation $\stvtilde$ is sampled and quantized with a period $T$, resulting in the digital control signal $\skv$ which is passed on to the digital estimator. The D/A converter is in this thesis assumed to be a non-return to zero (NRZ) DAC generating the control contribution $\stv$.

The digital control is called effective if it manages to keep the state vector bounded, given a bounded input. The input vector $\utv$ is bounded if it satisfies
\begin{equation}
    ||\utv||_{\infty} \leq b_{\bm{u}} \ \forall t.
\end{equation}
Equivalently, the state vector $\xtv$ is bounded if it satisfies
\begin{equation}
    \label{eq:bx}
    ||\xtv||_{\infty} \leq b_{\bm{x}} \ \forall t.
\end{equation}
In this thesis, the input signal will always be assumed bounded, and the boundary $b_{\bm{u}}$ is assumed to be determined by the application. The boundary for the state vector, $b_{\bm{x}}$, is a free variable and determines the magnitude of the state vector of the analog system.

A thorough analysis of the criteria for an effective control is found in \cite{malmberg_thesis}. The analysis is useful for the theoretical understanding of the system, but not necessary for the design process and is therefore beyond the scope of this paper. Intuitively however, there are three quantities affecting the stability of the analog system. The sampling period $T$ of the digital control, the unity gain frequency of the analog system and the boundary $b_{\bm{x}}$. Increasing the speed of the analog system would require a shorter sampling period to counteract the faster growth of the system states. Reducing the boundary $b_{\bm{x}}$ would require either reducing the speed of the analog system or increasing the sampling frequency, in order to maintain a tighter bound.

It will become apparent in the next section that the performance of the overall ADC is related to the digital controls ability to bound the state vector. Designing the ADC for a stability guarantee means that it is theoretically impossible for the state vector to grow beyond $b_{\bm{x}}$ at any point in time, given any valid input signal. This will of course result in a large stability margin most of the time, which means that there is potential for increased performance not being utilized. The preferred way of tuning the stability of the system is therefore through simulations, and then to include the possibility of a full system reset if it happens to become unstable.




















\section{Digital Estimator}
The digital estimator (DE) forms an estimate $\utvhat$ of $\utv$ based on the control signals $\skv$ and the knowledge of the analog systems parameters. The purpose of this section is to describe the digital estimation problem, and to present the optimum linear estimation filter.

\subsection{Statistical Estimation Problem and Transfer Functions}
In the following analysis, the system described by (\ref{eq:state_space_equations}) is assumed to be invariant and stable. This assumption is only needed to derive an analytic transfer function expression, and will not limit the actual estimation filter.

The objective of the digital estimator is to construct a digital estimate $\utvhat$ of $\utv$, based on the control signals $\skv$. As highlighted previously in this chapter, the direct relation between $\skv$ and $\utv$ is ignored completely. Instead, $\skv$ is only treated as the signal needed to stabilize the analog system, when triggered by an input signal $\utv$.

To formalize this approach, let $\ytvbreve \triangleq (\bm{g} * \bm{u})(t) \in \R^{\tilde{N}}$ be the signal that would have occurred at the output of the analog system in the absence of any digital control. Futhermore, let $\qtv$ be the control contribution signal seen at the output of the analog system. Because the control contribution enters the analog system in an additive way, their relation may be expressed as
\begin{equation}
    \label{eq:actual_observation}
    \ytv = \ytvbreve -\qtv.
\end{equation}
\begin{sidewaysfigure}
    \centering
    \input{figures/03theory/de_problem.tex}
    \caption{Block diagram of the complete control-bounded ADC, with the digital estimation problem visualized. The approximation $\ytv \approx \bm{0}$ is indicated by the fixed observation of $\tilde{\bm{y}}(t) = \bm{0}$ outside the DE box.}
    \label{fig:de_problem}
\end{sidewaysfigure}

The situation is illustrated in figure \ref{fig:de_problem}. In this figure, solid lines represent the physical components of the ADC, while dashed lines represents conceptual quantities that only exist inside the digital estimator. It is illustrated how $\qtv$ relates to the control contribution $\stv$. Because the digital estimator knows the parametrization of the analog system, as well as the waveform of the D/A converter, $\qtv$ is (in principle) known from the observation of $\skv$. Note that this illustration is only meant to illustrate the estimation problem of the digital estimator, not to show how the actual estimate is computed. We denote the frequency response of the digital estimation filter by $\Hw$ and the continuous time estimate $\utvhat$ of $\utv$ is obtained by
\begin{equation}
    \label{eq:uhat_conv_h_q}
    \utvhat = (\bm{h} * \bm{q})(t) \in \R^{L}.
\end{equation}

Because the objective of the analog system is to greatly amplify the sought frequency content of $\utv$, both $||\ytvbreve||_{\infty}$ and $||\qtv||_{\infty}$ will be very large compared to $||\ytv||_{\infty}$, which is bounded due to (\ref{eq:bx}). We can therefore approximate $\ytvbreve \approx \qtv$ which is equivalent to the approximation $\ytv \approx \bm{0}$. Hence the estimate may be written as
\begin{align}
    \utvhat &= (\bm{h} * \bm{q})(t) \\
            &= (\bm{h} * \breve{\bm{y}})(t) - (\bm{h} * \bm{y})(t) \label{eq:uhat_q_err}\\
            &\approx (\bm{h} * \breve{\bm{y}})(t) \label{eq:uhat_approx}\\
            &= (\bm{h} * \bm{g} * \bm{u})(t) \label{eq:uhat_q_err2}
\end{align}

It is evident that $\utvhat$ could have been computed with arbitrary accuracy, if the output $\ytv$ was completely known to the digital estimator. This if statement is obvious, but it illustrates an important point. Instead of relying on inevitably inaccurate measurement of $\ytv$ at fixed points in time, we choose to approximate this signal as being zero for all times $t$. The accuracy of the estimate then relies on the validity of the approximation $\ytv \approx \bm{0}$, rather than the precision of a direct measurement of $\ytv$ at some sampling points. This approximation is illustrated in figure \ref{fig:de_problem}, where it is indicated that the actual analog system output is disregarded and substituted with the fictional observation $\ytvtilde = \bm{0}$.

Any deviation of $\ytv$ from $\bm{0}$ will result in a conversion error, meaning that $\ytv$ is the conversion error signal seen at the output of the analog system. This conversion error does not enter the estimate directly, but is filtered by $\htv$. From the Fourier transform of (\ref{eq:uhat_q_err2}),
\begin{equation}
    \label{eq:F_uhat_err2}
    \hat{\bm{U}}(\omega) =  \underbrace{\Hw \Gw}_\text{STF}\bm{U}(\omega) - \underbrace{\Hw}_\text{NTF}\bm{Y}(\omega),
\end{equation}
we recognize the signal and noise transfer functions as $\text{STF} = \Hw \Gw$ and $\text{NTF} = \Hw$ respectively.

\subsubsection*{The Statistical Estimation Problem}
Up to this point, the focus has been to describe the context and the operating principle of the digital estimator. To derive an expression for the actual estimation filter, the problem is first described as a statistical estimation problem. The derivation is carried out in detail in \cite{malmberg_thesis} and only the main results is presented in this section.

The error introduces by the approximation in (\ref{eq:uhat_approx}) is treated by using statistical methods. In the following, both $\ytv$ and $\utv$ is assumed to be independent, centered, multivariate and wide-sense stationary stochastic processes. The estimation filter is then determined by
\begin{align}
    \htv    & = \argmin_{\bar{\bm{h}}} \E[(\utvhat - \utv)^2] \\
            & = \argmin_{\bar{\bm{h}}} \E[((\bar{\bm{h}}*\bm{q})(t) - \utv)^2].
\end{align}
This minimization problem is exactly the objective of the Wiener-filter \cite{optimal_filtering} and the impulse response matrix is given by the solution to the well-known Wiener-Hopf equations:
\begin{equation}
    \label{eq:wiener_hopf}
    (\bm{h}* \RqqT)(\tau) = \RuqT(-\tau)
\end{equation}
where
\begin{align}
    \RqqT & \triangleq \E[\qtv\bm{q}(t+\tau)^\mathsf{T}] \\
    \RuqT & \triangleq \E[\utv\bm{q}(t+\tau)^\mathsf{T}]
\end{align}
are the autocovariance and cross-covariance matrices respectively. By taking the Fourier transform of (\ref{eq:wiener_hopf}) we obtain the frequency response matrix $\Hw$ as
\begin{equation}
    \label{eq:Hw}
    \Hw = \GwH \left( \Gw\GwH + \eta^2\eye_N \right)^{-1},
\end{equation}
and the reader is referred to \cite{malmberg_thesis} for computational details.
The parameter $\eta$ is defined as
\begin{equation}
    \label{eq:def_eta}
    \eta \triangleq \frac{\sigma_{\bm{y}}^2}{\sigma_{\bm{u}}^2},
\end{equation}
where $\sigma_{\bm{y}}^2$ and $\sigma_{\bm{u}}^2$ are the power spectral densities of $\ytv$ and $\utv$ respectively.




\subsection{Estimation filter implementation}
With the digital estimation filter described by (\ref{eq:Hw}), the estimation could in principle be carried out by computing $\utvhat$ as in (\ref{eq:uhat_conv_h_q}). This computation is however not straight forward. First of all, the elements of $\qtv$ will necessarily be very large in magnitude, as this was the condition for the approximation (\ref{eq:uhat_approx}). Carrying out a continuous time convolution with this unbounded signal would obviously lead to numerical problems. In addition the computation of $\qtv$ from $\skv$, as illustrated in figure \ref{fig:de_problem}, might be computationally expensive.

In \cite{cbc_2011_loeliger} it was shown that the estimate $\utvhat$ can be computed in an alternative way, using a non-standard version of the Kalman smoothing algorithm. This algorithm converges to the the estimate (\ref{eq:uhat_conv_h_q}) as the considered time window extends towards infinity. The algorithm is also indifferent to the stability assumptions made in the previous section.

As the algorithm is nothing more than an efficient way of computing (\ref{eq:uhat_conv_h_q}), a description of the implementation is not needed for understanding the behaviour of the digital estimator. It is however important for simulations and a concise description of the filter algorithm is provided in appendix \ref{a:de_filter_description}.









\subsection{Practical Remarks}
\label{sec:practical_remarks}
We conclude this section with some practical considerations.

\subsubsection*{Controlling the Filter Bandwidth}
In (\ref{eq:def_eta}) the parameter $\eta$ was defined in terms of the power spectral densities of $\ytv$ and $\utv$, when these signals are modeled as independent stochastic processes. In practice however, $\eta$ is a free variable and is used by the designer to control the bandwidth of the estimation filter. To see this, consider the scalar input case where both $\Gw$ and $\Hw$ are column vectors. In this case, the noise transfer function (\ref{eq:Hw}) reduces to
\begin{equation}
    \label{eq:atf_scalar}
    \Hw = \text{NTF} = \frac{\GwH}{||\Gw||_2^2 + \eta^2} \in \C^{1 \times \tilde{N}},
\end{equation}
and the signal transfer function becomes
\begin{equation}
    \label{eq:stf_scalar}
    \text{STF} = \frac{||\Gw||_2^2}{||\Gw||_2^2 + \eta^2} \in \R.
\end{equation}
Assuming $||\Gw||_{\infty}$ is monotonically decreasing in $\omega$, the bandwidth of the digital estimator may be defined in terms of the critical frequency, $\omega_{c}$, as
\begin{equation}
    \label{eq:def_BW}
    ||\bm{G}(\omega_c)||_2^2 = \eta^2.
\end{equation}


As the parameter $\eta$ appeared with a precise definition from the optimum filter derivation, it might sound strange that this parameter is now treated as a free variable. This may be understood as follows. In the derivation that lead to (\ref{eq:def_eta}), no assumptions where made on the bandwidth of the input signal $\utv$. If $||\Gw||_{\infty}$ is monotonically decreasing in $\omega$, then above a certain frequency, the magnitude of the error signal, $\ytv$, and the input signal, $\utv$, will become comparable. Beyond this frequency, $\qtv$ contains more error than information, and the quality of the estimate is improved by reducing the influence of these higher frequency components. Therefore, with no prior knowledge of $\utv$, the optimum \enquote{cut-off} frequency of the estimation filter is given by $||\bm{G}(\omega_c)||_2 =\nicefrac{\sigma_{\bm{y}}^2}{\sigma_{\bm{u}}^2}.$

In a practical application however, we usually know which frequency components of $\utv$ that contains the sought information. In this case the quality of the estimate would obviously be improved by choosing the cut-off frequency based on this prior knowledge.



\subsubsection*{Signal-to-Noise Ratio}
An analytic derivation of the SNR of the control-bounded ADC is given in \cite{cbc_2020_loeliger}. The analysis models the output of the analog system, $\ytv$, as white noise, i.e. assuming the power spectral density is given by $\bm{S}_{\bm{y}\Tr{\bm{y}}}(\omega) \approx \sigma_{\bm{y}|\mathcal{B}}^2\eyen{\tilde{N}}$. In this expression, $\mathcal{B}$ denotes the frequency band of interest and $\sigma_{\bm{y}|\mathcal{B}}^2$ is the variance of $\ytv$ within this frequency band. From this assumption an approximated expression for the SNR is obtained as
\begin{equation}
    \label{eq:analytic_snr_est}
    \text{SNR} \approx \frac{\sigma_{\bm{y}|\mathcal{B}}^2}{2 \pi} \int_{\omega \in \mathcal{B}} \frac{1}{\norm{\Gw}{2}^2} d\omega.
\end{equation}

Even though this is an approximation it reveals a useful intuition of how the quantities affect the performance of the ADC. $\sigma_{\bm{y}|\mathcal{B}}^2$ relates to the magnitude of $\ytv$, and is minimized by tightening the control bound $b_{\bm{x}}$. $\norm{\Gw}{2}^2$ is maximized by increasing the gain of the analog system. Therefore, a tight control bound together with a high analog system gain result in large SNR.

The SNR is also related to the bandwidth parameter $\eta$, as seen by considering the ratio between the STF and NTF
\begin{align}
    \frac{\text{STF}(\omega_c)}{||\bm{H}(\omega_c)||_2} &= \frac{\norm{\bm{G}(\omega_c)}{2}^2}{\norm{\bm{G}(\omega_c)}{2}^2 + \eta^2 } \left( \frac{\norm{\bm{G}(\omega_c)}{2}}{\norm{\bm{G}(\omega_c)}{2}^2 + \eta^2 } \right)^{-1} \label{eq:stf_eta_start}\\
    & = \norm{\bm{G}(\omega_c)}{2} \\ &= \eta \label{eq:stf_eta_end}.
\end{align}
Therefore a trade-off has to be made between the bandwidth of the ADC and the suppression of the conversion error. This is similar to the trade-off in a $\sd$ ADC when considering the cut-off frequency of the decimation filter. This trade-off will be exemplified when particular ADC implementations is considered in the following chapters.

